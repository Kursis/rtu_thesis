{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.utils import Bunch\n",
    "import numpy as np\n",
    "from scipy.io.arff import loadarff\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import datetime as dt\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log(f):\n",
    "#     def wrapper(df, *args, **kwargs):\n",
    "#         t1 = dt.datetime.now()\n",
    "#         result = f(df, *args, **kwargs)\n",
    "#         t2 = dt.datetime.now()\n",
    "#         print(f\"{f.__name__} took {t2 - t1}, shape={len(result)}\")\n",
    "#         return result\n",
    "#     return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, y_fld=None, skip_flds=None,ignore_flds=None, do_scale=False, \n",
    "                na_dict=None, preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n",
    "    if not ignore_flds: ignore_flds=[]\n",
    "    if not skip_flds: skip_flds=[]\n",
    "    if subset: df = get_sample(df,subset)\n",
    "    else: df = df.copy()\n",
    "    ignored_flds = df.loc[:, ignore_flds]\n",
    "    df.drop(ignore_flds, axis=1, inplace=True)\n",
    "    if preproc_fn: preproc_fn(df)\n",
    "    if y_fld is None: y = None\n",
    "    else:\n",
    "        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n",
    "        y = df[y_fld].values\n",
    "        skip_flds += [y_fld]\n",
    "    df.drop(skip_flds, axis=1, inplace=True)\n",
    "\n",
    "    if do_scale: mapper = scale_vars(df, mapper)\n",
    "    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n",
    "    df = pd.get_dummies(df, dummy_na=True)\n",
    "    df = pd.concat([ignored_flds, df], axis=1)\n",
    "    res=[df,y]\n",
    "    if do_scale: res = res + [mapper]\n",
    "    return res\n",
    "\n",
    "def numericalize(df, col, name, max_n_cat):\n",
    "    if not is_numeric_dtype(col) and (max_n_cat is None or len(col.cat.categories)>max_n_cat):\n",
    "        df[name] = pd.Categorical(col).codes+1\n",
    "\n",
    "def process_categoricals(df):\n",
    "    for n,c in df.items():\n",
    "        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_pipeline(df):\n",
    "    df = (df\n",
    "          .pipe(process_categoricals)\n",
    "          .pipe(encode_data)[0]\n",
    "            )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_crime():\n",
    "    feature_names = pd.read_csv(f\"raw_datasets/crime/communities.names\", sep=\"\\t\", \n",
    "                                header=None, index_col=None)[57:185]\n",
    "    feature_names = feature_names.replace('@attribute', '', regex=True)\n",
    "    feature_names = feature_names.replace('numeric', '', regex=True)\n",
    "    feature_names = feature_names.replace('string', '', regex=True)\n",
    "    feature_names = feature_names.reset_index(drop=True)\n",
    "    \n",
    "    # remove trailing/leading whitespace, cast to np arr\n",
    "    feature_names = feature_names.apply(lambda x: x.str.strip())\n",
    "    feature_names = list(feature_names[0])\n",
    "    feature_names = np.array(feature_names)\n",
    "\n",
    "    df = pd.read_csv('raw_datasets/crime/communities.data', sep=',',\n",
    "                     header=None, names=feature_names)\n",
    "    df = default_pipeline(df)\n",
    "    # drop debug columns\n",
    "    df = df.drop(columns = df.columns[:5])\n",
    "    data_bunch = Bunch(\n",
    "             data=df.drop('ViolentCrimesPerPop', axis=1).to_numpy(),\n",
    "             target=np.array(df.ViolentCrimesPerPop),\n",
    "             feature_names=feature_names[5:-1]\n",
    "                        )\n",
    "    return data_bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ames_housing():\n",
    "    data = pd.read_csv('raw_datasets/ames_housing/ames.txt', sep='\\t')\n",
    "    data = data.drop(['Order', 'PID'], axis=1)\n",
    "    target = data.SalePrice\n",
    "    data = data.drop('SalePrice', axis=1)\n",
    "    feature_names = data.columns\n",
    "    feature_names = np.array(feature_names)\n",
    "    \n",
    "    data = default_pipeline(data)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    feature_names = np.array(feature_names)\n",
    "    target = np.array(target)\n",
    "    data_bunch = Bunch(\n",
    "             data=data,\n",
    "             target=target,\n",
    "             feature_names=feature_names\n",
    "                        )\n",
    "    return data_bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_phishing():\n",
    "    phishing, _ = loadarff('raw_datasets/phishing/phishing.arff')\n",
    "    feature_names = phishing.dtype.names\n",
    "    feature_names = np.array(feature_names)\n",
    "    df = pd.DataFrame(phishing).astype('int')\n",
    "    \n",
    "    df = default_pipeline(df)\n",
    "    \n",
    "    data = df.drop('Result', axis=1)\n",
    "    data = np.array(data)\n",
    "    target = df.Result\n",
    "    target = np.array(target)\n",
    "    # xgboost sucks and wants you to give labels in in the range of [0, inf] \n",
    "    # instead of [-1, inf]\n",
    "    target += 1\n",
    "    data += 1\n",
    "    # xgboost sucks and wants you the class labels to be in order\n",
    "    target[target == 2] = 1\n",
    "    data_bunch = Bunch(\n",
    "             data=data,\n",
    "             target=target,\n",
    "             feature_names=feature_names[:-1]\n",
    "                        )    \n",
    "    return data_bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mushroom():\n",
    "    df = pd.read_csv('raw_datasets/mushrooms/mushrooms.csv', sep=',')\n",
    "    df = default_pipeline(df)\n",
    "    \n",
    "    target = np.array(df['class'])\n",
    "    data = df.drop('class', axis=1)\n",
    "    feature_names = data.columns\n",
    "    data = np.array(data)\n",
    "    feature_names = np.array(feature_names)\n",
    "    # xgboost sucks and wants you the class labels to be in order\n",
    "    target -= 1\n",
    "    data_bunch = Bunch(\n",
    "             data=data,\n",
    "             target=target,\n",
    "             feature_names=feature_names\n",
    "                        )\n",
    "    return data_bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ames_housing():\n",
    "    data = pd.read_csv('raw_datasets/ames_housing/ames.txt', sep='\\t'\n",
    "                      )\n",
    "    data = data.drop(['Order', 'PID'], axis=1)\n",
    "    \n",
    "    # I would recommend removing any houses with more than 4000 square feet \n",
    "    # from the data set (which eliminates these 5 unusual observations)\n",
    "    # before assigning it to students.\n",
    "    # source: http://jse.amstat.org/v19n3/decock/DataDocumentation.txt\n",
    "    # section: SPECIAL NOTES\n",
    "    data = data[data['Gr Liv Area'] < 4000]\n",
    "    \n",
    "    target = data.SalePrice\n",
    "    data = data.drop('SalePrice', axis=1)\n",
    "    feature_names = data.columns\n",
    "    data = default_pipeline(data)\n",
    "    \n",
    "    # todo: check if this is ok, currently \n",
    "    # it seems that our pipeline does not parse float dtypes\n",
    "    # fill float dtypes with -999 (out of distribution value)\n",
    "    data = data.fillna(-999)\n",
    "    data = np.array(data)\n",
    "    feature_names = np.array(feature_names)\n",
    "    target = np.array(target)\n",
    "    data_bunch = Bunch(\n",
    "             data=data,\n",
    "             target=target,\n",
    "             feature_names=feature_names\n",
    "                        )\n",
    "    return data_bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test \n",
    "# a = read_crime()\n",
    "# b = read_ames_housing()\n",
    "# c = read_mushroom()\n",
    "# d = read_phishing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
